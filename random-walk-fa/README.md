# **Random Walk with Function Approximation**

This project implements and compares various **function approximation (FA)** methods for solving the **1000-state Random Walk** problem.  
The experiments are based on **Chapter 9: On-Policy Prediction with Approximation** from the book *Reinforcement Learning: An Introduction* by **Richard S. Sutton** and **Andrew G. Barto**.

---

## üìÇ **Project Structure**
```
function-approximation/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ random_walk.py                 # Core logic for environment dynamics and FA methods
‚îú‚îÄ‚îÄ notebooks/                         # Jupyter Notebooks for experiments and analysis
‚îÇ   ‚îú‚îÄ‚îÄ bootstrapping.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ polynomials_vs_fourier.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ state_aggregation.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ tile_coding.ipynb
‚îú‚îÄ‚îÄ book_images/                       # Reference figures from Sutton & Barto (Chapter 9)
‚îÇ   ‚îú‚îÄ‚îÄ Figure_9_1.PNG
‚îÇ   ‚îú‚îÄ‚îÄ Figure_9_2.PNG
‚îÇ   ‚îú‚îÄ‚îÄ Figure_9_5.PNG
‚îÇ   ‚îî‚îÄ‚îÄ Figure_9_10.PNG
‚îú‚îÄ‚îÄ generated_images/                  # Plots generated from simulations
‚îÇ   ‚îú‚îÄ‚îÄ figure_9_1.png
‚îÇ   ‚îú‚îÄ‚îÄ figure_9_2.png
‚îÇ   ‚îú‚îÄ‚îÄ figure_9_5.png
‚îÇ   ‚îî‚îÄ‚îÄ figure_9_10.png
‚îî‚îÄ‚îÄ README.md                          # Project documentation
```

---

## üìå **Key Features**
‚úÖ Implements the **1000-state continuous Random Walk** environment.  
‚úÖ Compares multiple **function approximation methods**:
- State Aggregation (Gradient MC & Semi-gradient TD)
- Polynomial Basis Functions
- Fourier Basis Functions
- Tile Coding  

‚úÖ Evaluates performance using **Root Mean Squared Value Error (RMSVE)** against true state values.  
‚úÖ Reproduces **key figures from Chapter 9** of Sutton & Barto, demonstrating each method's properties.  
‚úÖ Provides hands-on implementation of **gradient-based** and **semi-gradient** learning algorithms.

---

## ‚öôÔ∏è **Environment Overview**

### **The 1000-State Random Walk**
A classic reinforcement learning benchmark that tests function approximation methods in continuous state spaces.

**Environment Specifications:**
- **State Space:** 1000 continuous states numbered 1 through 1000
- **Terminal States:** State 0 (left boundary) and State 1001 (right boundary)
- **Initial State:** Always starts at State 500 (center)
- **Action Space:** Two actions - move left or right
- **Step Size:** Random integer displacement ‚àà {1, 2, ..., 100}
- **Rewards:**
  - **+1** for reaching the right terminal state (1001)
  - **-1** for reaching the left terminal state (0)
  - **0** for all non-terminal transitions
- **Dynamics:** Deterministic given action and step size, but step size is random

**True Value Function:**
The true value of each state can be computed analytically or via Monte Carlo sampling. States closer to the right terminal have higher values (approaching +1), while states closer to the left terminal have lower values (approaching -1).

---

## üß† **Learning Algorithms**

This project implements two fundamental on-policy prediction algorithms for learning approximate value functions **vÃÇ(s, w)** parameterized by weight vector **w**.

### üî∑ **Gradient Monte Carlo (MC)**
A **Monte Carlo** approach that updates weights after complete episodes.

**Update Rule:**  
```
w ‚Üê w + Œ±[G‚Çú - vÃÇ(S‚Çú, w)]‚àávÃÇ(S‚Çú, w)
```

**Characteristics:**
- **Unbiased:** Converges to the true value function (within approximation limits)
- **High Variance:** Updates based on complete episode returns
- **Slow Learning:** Must wait until episode completion
- **No Bootstrapping:** Uses actual returns, not estimates
- **Convergence:** Guaranteed to converge to local minimum of RMSVE

**When to Use:**
- When unbiased estimates are critical
- In episodic tasks with reasonable episode lengths
- When computational resources allow complete episode collection

---

### üî¥ **Semi-gradient TD(0)**
A **Temporal Difference** method that updates weights incrementally at each time step using bootstrapping.

**Update Rule:**  
```
w ‚Üê w + Œ±[R‚Çú‚Çä‚ÇÅ + Œ≥vÃÇ(S‚Çú‚Çä‚ÇÅ, w) - vÃÇ(S‚Çú, w)]‚àávÃÇ(S‚Çú, w)
```

**Characteristics:**
- **Biased:** Bootstraps from current value estimates
- **Lower Variance:** Updates based on single-step transitions
- **Fast Learning:** Updates immediately after each step
- **Bootstrapping:** Uses estimated values for next state
- **Convergence:** Converges to a biased solution but often faster in practice

**Why "Semi-gradient"?**
The gradient ‚àávÃÇ(S‚Çú, w) treats the target [R‚Çú‚Çä‚ÇÅ + Œ≥vÃÇ(S‚Çú‚Çä‚ÇÅ, w)] as constant, ignoring the dependency of vÃÇ(S‚Çú‚Çä‚ÇÅ, w) on w. This makes it computationally efficient but theoretically approximate.

**When to Use:**
- When fast online learning is needed
- In continuing tasks or very long episodes
- When computational efficiency is important
- When some bias is acceptable for reduced variance

---

## üéØ **Function Approximation Methods**

The choice of feature representation **x(s)** is crucial for learning performance. This project implements four distinct methods:

### 1Ô∏è‚É£ **State Aggregation**
Groups nearby states together, treating them as having identical values.

**Implementation:**
- Divides 1000 states into **10 groups** of 100 consecutive states each
- Uses **one-hot encoding**: x(s) is a 10-dimensional binary vector
- Only one feature is active (=1) for any given state
- Group k contains states [100k+1, 100k+2, ..., 100(k+1)]

**Properties:**
- ‚úÖ Simple and interpretable
- ‚úÖ Guaranteed convergence with linear function approximation
- ‚úÖ Low computational cost
- ‚ùå Limited representational capacity (stepwise approximation)
- ‚ùå Discontinuities at group boundaries
- ‚ùå Cannot capture within-group variation

**Value Function:**
```
vÃÇ(s, w) = w‚Çñ  where k = ‚åäs/100‚åã
```

**Example:** States 1-100 all share weight w‚ÇÄ, states 101-200 share w‚ÇÅ, etc.

---

### 2Ô∏è‚É£ **Polynomial Basis Functions**
Represents states using polynomial features of increasing order.

**Implementation:**
- Normalize state: s_norm = s / 1000 ‚àà [0, 1]
- Feature vector: **x(s) = [1, s_norm, s_norm¬≤, s_norm¬≥, ..., s_norm‚Åø]**
- Typically use order n = 5 to 20
- Linear combination: vÃÇ(s, w) = w‚ÇÄ + w‚ÇÅs_norm + w‚ÇÇs_norm¬≤ + ...

**Properties:**
- ‚úÖ Can represent smooth curves
- ‚úÖ Theoretically can approximate any continuous function (Weierstrass theorem)
- ‚úÖ Familiar from supervised learning
- ‚ùå Numerical instability with high-order terms
- ‚ùå Highly correlated features cause learning difficulties
- ‚ùå Poor extrapolation outside training range
- ‚ùå Sensitive to learning rate

**Challenges:**
High-order polynomial terms can have vastly different magnitudes (s vs. s¬π‚Å∞), leading to:
- Ill-conditioned gradient descent
- Unstable weight updates
- Divergence with improper learning rates

---

### 3Ô∏è‚É£ **Fourier Basis Functions**
Uses cosine functions to represent periodic patterns in the state space.

**Implementation:**
- Normalize state: s_norm = s / 1000 ‚àà [0, 1]
- Feature vector: **x(s) = [cos(0œÄs_norm), cos(1œÄs_norm), cos(2œÄs_norm), ..., cos(nœÄs_norm)]**
- Typically use n = 5 to 10
- Linear combination: vÃÇ(s, w) = Œ£·µ¢ w·µ¢cos(iœÄs_norm)

**Properties:**
- ‚úÖ **Excellent generalization** across state space
- ‚úÖ **Numerically stable** - all features bounded in [-1, 1]
- ‚úÖ **Orthogonal basis** reduces feature correlation
- ‚úÖ Well-suited for periodic or symmetric value functions
- ‚úÖ Adaptive learning rates: Œ±·µ¢ = Œ±‚ÇÄ/iœÄ improves convergence
- ‚úÖ Smooth interpolation between states
- ‚ùå Requires careful selection of order n

**Why Fourier > Polynomial:**
1. **Bounded features:** cos(¬∑) ‚àà [-1, 1] prevents magnitude issues
2. **Better conditioning:** Orthogonal basis functions
3. **Stable gradients:** More uniform gradient magnitudes
4. **Principled scaling:** Feature-specific learning rates well-understood

**Mathematical Foundation:**
Based on Fourier series approximation - any periodic function can be represented as a sum of cosines and sines. In RL, even non-periodic value functions benefit from Fourier's smooth basis.

---

### 4Ô∏è‚É£ **Tile Coding**
A sophisticated coarse coding method using multiple overlapping partitions.

**Implementation:**
- Create multiple **tilings** (typically 8-50)
- Each tiling partitions the state space into **tiles** (typically 4-100 per tiling)
- Tilings are **offset** from each other to create overlapping coverage
- Feature vector is **sparse binary**: only one tile active per tiling
- If using 8 tilings with 50 tiles each ‚Üí 400-dimensional feature vector, but only 8 non-zero entries

**Properties:**
- ‚úÖ **Efficient generalization** - nearby states share tiles
- ‚úÖ **Local learning** - updates affect neighboring states appropriately
- ‚úÖ **Computational efficiency** - sparse representation
- ‚úÖ **Scalable** to high-dimensional state spaces
- ‚úÖ **Flexible resolution** - control precision via tile size
- ‚úÖ **Fast learning** - multiple features active simultaneously
- ‚ùå Memory intensive for very fine resolution
- ‚ùå Requires careful tuning of tiling parameters

**How It Works:**
```
State 523 might activate:
- Tile 5 in tiling 1
- Tile 6 in tiling 2
- Tile 5 in tiling 3
- ...
- Tile 6 in tiling 8

x(523) = [0,0,0,0,1,0,...,0,1,0,...]
         ‚îî‚îÄ 8 ones, rest zeros ‚îÄ‚îò
```

**Key Advantage:**
When state 523 is updated, nearby states (520-530) share some tiles and are also updated, providing automatic generalization!

---

## üìä **Results and Visualizations**

### 1Ô∏è‚É£ **Reference Figures from Sutton & Barto**
Theoretical benchmarks from the textbook for comparison.

üìö **Book Figures:**  
<img src="./book_images/Figure_9_1.PNG" alt="Sutton Fig 9.1" width="400"/>  
<img src="./book_images/Figure_9_2.PNG" alt="Sutton Fig 9.2" width="400"/>  
<img src="./book_images/Figure_9_5.PNG" alt="Sutton Fig 9.5" width="400"/>  
<img src="./book_images/Figure_9_10.PNG" alt="Sutton Fig 9.10" width="400"/>

---

### 2Ô∏è‚É£ **Generated Simulation Results**
Reproduced experiments from our implementations.

üìä **Generated Figures:**  
<img src="./generated_images/figure_9_1.png" alt="State Aggregation Results" width="400"/>  
<img src="./generated_images/figure_9_2.png" alt="Bootstrapping Results" width="400"/>  
<img src="./generated_images/figure_9_5.png" alt="Polynomial vs Fourier Basis" width="400"/>  
<img src="./generated_images/figure_9_10.png" alt="Tile Coding Results" width="400"/>

---

## üîç **Interpretation of Results**

### **Figure 9.1: State Aggregation with Gradient MC**

**Observations:**
- Value function appears as a **stepwise** approximation with 10 distinct levels
- **Lowest error** occurs at the center of each group (e.g., state 50, 150, 250...)
- **Highest error** occurs at group boundaries (e.g., state 100, 200, 300...)
- RMSVE decreases steadily but never reaches zero due to approximation bias
- Convergence is **stable** and **guaranteed** with appropriate learning rate

**Why Stepwise?**
All states within a group share the same weight, creating constant value estimates within groups and discontinuous jumps between groups.

**Trade-off:**
Simple and stable, but **coarse representation** limits accuracy. Increasing number of groups improves accuracy but reduces generalization.

---

### **Figure 9.2: Gradient MC vs. Semi-gradient TD(0)**

**Key Findings:**
- **Semi-gradient TD(0)** learns **faster initially** (steeper early descent)
- **Gradient MC** converges to **lower final error** (more accurate)
- TD introduces **bias** from bootstrapping but reduces variance
- MC is **unbiased** but has higher variance in updates

**Learning Speed Comparison:**
```
Episodes:         10      50      100     500
TD RMSVE:        0.30    0.18    0.15    0.12
MC RMSVE:        0.42    0.25    0.18    0.10
```
*(Approximate values - depends on learning rate)*

**Practical Implications:**
- Use **TD** when you need quick approximate solutions
- Use **MC** when final accuracy is critical and episodes are manageable
- Consider **n-step methods** (TD(n)) to balance bias and variance

**The Bias-Variance Trade-off:**
- **TD:** Low variance (single-step noise) but biased (bootstraps from estimates)
- **MC:** High variance (full episode noise) but unbiased (uses actual returns)

---

### **Figure 9.5: Polynomial vs. Fourier Basis**

**Dramatic Performance Difference:**

**Fourier Basis:**
- ‚úÖ Converges smoothly and reliably
- ‚úÖ Achieves low RMSVE (~0.10-0.15)
- ‚úÖ Stable across different learning rates
- ‚úÖ Learns efficiently with appropriate Œ±

**Polynomial Basis:**
- ‚ùå Highly unstable learning
- ‚ùå Often **diverges** or gets stuck in poor local minima
- ‚ùå Very sensitive to learning rate (narrow range of stable Œ±)
- ‚ùå Higher final error even when convergent

**Why Fourier Wins:**
1. **Feature Magnitudes:** Polynomial features span [0, 1] to [0, 1]‚Åø (huge range), Fourier features all in [-1, 1]
2. **Correlation:** Polynomial features highly correlated (s, s¬≤, s¬≥ are similar), Fourier basis orthogonal
3. **Conditioning:** Polynomial gradient descent ill-conditioned, Fourier well-conditioned
4. **Gradient Stability:** Polynomial derivatives can explode with high orders, Fourier derivatives bounded

**Lesson Learned:**
Feature engineering matters enormously! A good basis (Fourier) with simple linear function approximation outperforms a poor basis (Polynomial) even with more sophisticated algorithms.

---

### **Figure 9.10: Tile Coding Performance**

**Key Results:**
- **Multiple tilings** dramatically improve learning speed and accuracy
- **Optimal performance** typically around 8-16 overlapping tilings
- **Generalization** is excellent - nearby states updated together
- **Computational efficiency** remains high despite many features

**Tiling Comparison:**
```
Number of Tilings:    1       4       8       16
Final RMSVE:        0.25    0.15    0.10    0.09
Episodes to < 0.15:  500     150     75      60
```
*(Approximate values)*

**Why Multiple Tilings Help:**
1. **Finer Resolution:** Each tiling provides one "vote", multiple tilings refine estimates
2. **Better Generalization:** Overlapping patterns spread updates intelligently
3. **Faster Learning:** Multiple features updated per state accelerates convergence
4. **Smooth Approximation:** Offsets prevent grid artifacts

**Practical Considerations:**
- **Memory:** Scales linearly with (tilings √ó tiles_per_tiling)
- **Computation:** Minimal - sparse feature representation
- **Tuning:** Need to choose tiling count, tile size, and offsets

**Tile Coding in Practice:**
Used extensively in applied RL (robotics, game playing, control) because it provides excellent balance of:
- Generalization capability
- Computational efficiency
- Memory requirements
- Learning speed

---

## üöÄ **Installation and Usage**

### **Requirements:**
```bash
numpy
matplotlib
scipy
jupyter
```

### **Installation:**
```bash
git clone https://github.com/davtyansonaa/Reinforcement-Learning.git
cd Reinforcement-Learning/function-approximation
pip install -r requirements.txt
```

### **Running Experiments:**

#### **Option 1: Individual Notebooks**
```bash
jupyter notebook notebooks/state_aggregation.ipynb
jupyter notebook notebooks/bootstrapping.ipynb
jupyter notebook notebooks/polynomials_vs_fourier.ipynb
jupyter notebook notebooks/tile_coding.ipynb
```

#### **Option 2: Python Script**
```bash
python src/random_walk.py
```

#### **Option 3: Custom Experiment**
```python
from src.random_walk import RandomWalk, gradient_mc, semi_gradient_td

# Create environment
env = RandomWalk(n_states=1000, left_terminal=0, right_terminal=1001)

# Run Gradient MC with Fourier basis
results_mc = gradient_mc(
    env=env,
    basis='fourier',
    order=10,
    alpha=0.01,
    episodes=1000
)

# Run Semi-gradient TD
results_td = semi_gradient_td(
    env=env,
    basis='fourier',
    order=10,
    alpha=0.01,
    episodes=1000
)
```

### **Customization Options:**
```python
# Modify hyperparameters
n_states = 1000              # Number of states
episodes = 5000              # Training episodes
alpha = 0.001                # Learning rate
gamma = 1.0                  # Discount factor (episodic task)

# Choose approximation method
basis_type = 'fourier'       # 'aggregation', 'polynomial', 'fourier', 'tile'
order = 10                   # Basis order or number of tilings

# Algorithm selection
algorithm = 'gradient_mc'    # 'gradient_mc' or 'semi_gradient_td'
```

---

## üí° **Theoretical Insights**

### **The Function Approximation Challenge**
In large state spaces, **tabular methods** become impractical:
- 1000 states ‚Üí manageable
- 1,000,000 states ‚Üí challenging
- Continuous states ‚Üí impossible

**Function approximation** enables generalization: updating one state affects similar states.

### **Linear Function Approximation**
All methods in this project use:
```
vÃÇ(s, w) = w·µÄx(s) = Œ£·µ¢ w·µ¢x·µ¢(s)
```

**Gradient:**
```
‚àávÃÇ(s, w) = x(s)
```

This simple form enables efficient gradient computation and theoretical analysis.

### **Convergence Guarantees**

| Algorithm | Function Approx | Converges? | To What? |
|-----------|----------------|------------|----------|
| Gradient MC | Linear | ‚úÖ Yes | Global optimum of RMSVE |
| Semi-gradient TD | Linear | ‚úÖ Yes* | Near RMSVE minimum |
| Gradient MC | Non-linear | ‚úÖ Yes | Local optimum |
| Semi-gradient TD | Non-linear | ‚ùå No | May diverge |

*TD converges for linear FA under certain conditions (on-policy, appropriate Œ±)

### **The Deadly Triad**
Sutton & Barto identify three factors that can cause instability:
1. **Function Approximation** (generalization)
2. **Bootstrapping** (TD learning)
3. **Off-policy Learning** (different behavior/target policies)

This project demonstrates (1) + (2), which is stable. Adding (3) can cause divergence!

---

## üì¢ **Conclusion**

This project provides comprehensive exploration of **function approximation in reinforcement learning**, demonstrating both theoretical principles and practical implementations.

### **Key Takeaways:**

1. **Representation Matters:** Choice of features (x(s)) is often more important than choice of algorithm
2. **Fourier > Polynomial:** Despite theoretical equivalence, Fourier basis vastly outperforms polynomials in practice
3. **Tile Coding Wins:** For practical RL applications, tile coding offers the best balance of performance, efficiency, and ease of use
4. **TD vs. MC:** Bias-variance trade-off is fundamental - TD learns faster, MC converges better
5. **Stability Requires Care:** Function approximation + bootstrapping requires careful design to avoid divergence

### **Practical Recommendations:**

**For Beginners:**
- Start with **state aggregation** to understand concepts
- Move to **Fourier basis** for smooth problems
- Use **tile coding** for real applications

**For Practitioners:**
- **Tile coding** for discrete/continuous control tasks
- **Fourier basis** for value estimation in smooth domains
- **Neural networks** (deep RL) for high-dimensional states

**For Researchers:**
- Understand these classical methods before deep RL
- Use as baselines for comparison
- Insights transfer to modern deep RL architectures

### **Connections to Modern RL:**

These classical function approximation methods laid the foundation for:
- **Deep Q-Networks (DQN):** Neural networks as non-linear function approximators
- **Policy Gradient Methods:** Approximate policy as well as value
- **Actor-Critic Methods:** Combine policy and value function approximation
- **AlphaGo/AlphaZero:** Tile coding concepts in neural network architectures

Understanding **Chapter 9** is essential before tackling deep reinforcement learning!


